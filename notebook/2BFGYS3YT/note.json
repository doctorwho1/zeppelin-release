{"paragraphs":[{"text":"%md\n\n# Introduction to Spark with Python\n#### Using Spark RDDs and DataFrames to Analyze a Text File\n@RobHryniewicz\nver 0.5","dateUpdated":"Apr 15, 2016 1:19:50 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510741_-78307917","id":"20160331-233830_1876799966","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Introduction to Spark with Python</h1>\n<h4>Using Spark RDDs and DataFrames to Analyze a Text File</h4>\n<p>@RobHryniewicz\n<br  />ver 0.5</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Apr 15, 2016 1:19:47 PM","dateFinished":"Apr 15, 2016 1:19:47 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1580","focus":true},{"text":"%md\n## Introduction\n\nThis lab consists of two parts. In each section you will perform a basic Word Count.\n#\nIn Part 1, we will introduce RDDs, Spark's primary low-level abstraction, and several core concepts.\n#\nIn Part 2, we will introduce DataFrames, a higher-level abstraction than RDDs, along with SparkSQL allowing you to use SQL statements to query a temporary table.","dateUpdated":"Mar 31, 2016 11:40:00 PM","config":{"enabled":true,"graph":{"mode":"table","height":217,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510744_-79462163","id":"20160331-233830_1038788941","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Introduction</h2>\n<p>This lab consists of two parts. In each section you will perform a basic Word Count.</p>\n<h1></h1>\n<p>In Part 1, we will introduce RDDs, Spark's primary low-level abstraction, and several core concepts.</p>\n<h1></h1>\n<p>In Part 2, we will introduce DataFrames, a higher-level abstraction than RDDs, along with SparkSQL allowing you to use SQL statements to query a temporary table.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Mar 31, 2016 11:39:58 PM","dateFinished":"Mar 31, 2016 11:39:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1581"},{"text":"%md\n### Concepts\n\nAt the core of Spark is the notion of a Resilient Distributed Dataset (RDD), which is an immutable and fault-tolerant collection of objects that is partitioned and distributed across multiple physical nodes on a cluster and they run in parallel.\n#\nTypically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.\n#\nOnce an RDD is instantiated, you can apply a **[series of operations](https://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)**.\n#\nAll operations fall into one of two types: **[Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations)** or **[Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions)**. \n#\nTransformation operations, as the name suggests, create new datasets from an existing RDD and build out the processing Directed Acyclic Graph (DAG) that can then be applied on the partitioned dataset across the YARN cluster. An Action operation, on the other hand, executes DAG and returns a value.\n#\nIn this lab we will use the following **[Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations)**:\n- map(func)\n- filter(func)\n- flatMap(func)\n- reduceByKey(func)\n\nand **[Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions)**:\n\n- collect()\n- count()\n- take()\n- takeOrdered(n, [ordering])\n- countByKey()\n\nA typical Spark application has the following four phases:\n1. Instantiate Input RDDs\n2. Transform RDDs\n3. Persist Intermediate RDDs\n4. Take Action on RDDs","dateUpdated":"Apr 1, 2016 12:35:08 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510744_-79462163","id":"20160331-233830_2031164924","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Concepts</h3>\n<p>At the core of Spark is the notion of a Resilient Distributed Dataset (RDD), which is an immutable and fault-tolerant collection of objects that is partitioned and distributed across multiple physical nodes on a cluster and they run in parallel.</p>\n<h1></h1>\n<p>Typically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.</p>\n<h1></h1>\n<p>Once an RDD is instantiated, you can apply a <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#rdd-operations\">series of operations</a></strong>.</p>\n<h1></h1>\n<p>All operations fall into one of two types: <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#transformations\">Transformations</a></strong> or <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#actions\">Actions</a></strong>.</p>\n<h1></h1>\n<p>Transformation operations, as the name suggests, create new datasets from an existing RDD and build out the processing Directed Acyclic Graph (DAG) that can then be applied on the partitioned dataset across the YARN cluster. An Action operation, on the other hand, executes DAG and returns a value.</p>\n<h1></h1>\n<p>In this lab we will use the following <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#transformations\">Transformations</a></strong>:</p>\n<ul>\n<li>map(func)</li>\n<li>filter(func)</li>\n<li>flatMap(func)</li>\n<li>reduceByKey(func)</li>\n</ul>\n<p>and <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#actions\">Actions</a></strong>:</p>\n<ul>\n<li>collect()</li>\n<li>count()</li>\n<li>take()</li>\n<li>takeOrdered(n, [ordering])</li>\n<li>countByKey()</li>\n</ul>\n<p>A typical Spark application has the following four phases:</p>\n<ol>\n<li>Instantiate Input RDDs</li>\n<li>Transform RDDs</li>\n<li>Persist Intermediate RDDs</li>\n<li>Take Action on RDDs</li>\n</ol>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Apr 1, 2016 12:35:07 AM","dateFinished":"Apr 1, 2016 12:35:07 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1582"},{"text":"%md\n### Lab Pre-Check\nBefore we proceed, let's verify Spark Version. You should be running at minimum Spark 1.6.\n#\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. This may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running.","dateUpdated":"Mar 31, 2016 11:44:32 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510745_-79846912","id":"20160331-233830_1388824956","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Lab Pre-Check</h3>\n<p>Before we proceed, let's verify Spark Version. You should be running at minimum Spark 1.6.</p>\n<h1></h1>\n<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background. This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Mar 31, 2016 11:44:30 PM","dateFinished":"Mar 31, 2016 11:44:30 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1583"},{"text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"Mar 31, 2016 11:44:04 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510745_-79846912","id":"20160331-233830_981276249","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Mar 31, 2016 11:44:02 PM","dateFinished":"Mar 31, 2016 11:44:02 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1584"},{"title":"Check Spark Version","text":"sc.version","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510746_-78692666","id":"20160331-233830_1782991630","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1585"},{"text":"%md ####Now let's proceed with our core lab.\n","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510746_-78692666","id":"20160331-233830_122830635","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Now let's proceed with our core lab.</h4>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1586"},{"text":"%md \n\n## Part 1\n#### Introduction to RDDs with Word Count example","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510747_-79077415","id":"20160331-233830_682697678","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 1</h2>\n<h4>Introduction to RDDs with Word Count example</h4>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1587"},{"text":"%md\nIn this section you will perform a basic word count with RDDs.\n#\nYou will download external text data file to your sandbox. Then you will perform lexical analysis, or tokenization, by breaking up text into words/tokens.\nThe list of tokens then becomes an input for further processing to this and following sections.\n#\nBy the end of this section you should have learned how to perform low-level transformations and actions with Spark RDDs and lambda expressions.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510747_-79077415","id":"20160331-233830_94748225","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this section you will perform a basic word count with RDDs.</p>\n<h1></h1>\n<p>You will download external text data file to your sandbox. Then you will perform lexical analysis, or tokenization, by breaking up text into words/tokens.\n<br  />The list of tokens then becomes an input for further processing to this and following sections.</p>\n<h1></h1>\n<p>By the end of this section you should have learned how to perform low-level transformations and actions with Spark RDDs and lambda expressions.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1588"},{"text":"%md\nIn the next paragraph we are going to download data using shell commands. A shell command in a Zeppelin notebookcan can be invoked by \nprepending a block of shell commands with a line containing `%sh` characters.","dateUpdated":"Mar 31, 2016 11:45:28 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510747_-79077415","id":"20160331-233830_1148035148","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the next paragraph we are going to download data using shell commands. A shell command in a Zeppelin notebookcan can be invoked by\n<br  />prepending a block of shell commands with a line containing <code>%sh</code> characters.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Mar 31, 2016 11:45:27 PM","dateFinished":"Mar 31, 2016 11:45:27 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1589"},{"title":"Prepare Directories and Download a Dataset","text":"%sh\ncd /tmp\n\n#  Remove old dataset file if already exists in local /tmp directory\nif [ -e /tmp/About-Apache-NiFi.txt ]\nthen\n    rm -f /tmp/About-Apache-NiFi.txt\nfi\n\n# Remove old dataset if already exists in hadoop /tmp directory\nif hadoop fs -stat /tmp/About-Apache-NiFi.txt\nthen\n   hadoop fs -rm  /tmp/About-Apache-NiFi.txt\nfi\n\n# Download \"About-Apache-NiFi\" text file\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/About-Apache-NiFi.txt\n\n# Move dataset to hadoop /tmp\nhadoop fs -put About-Apache-NiFi.txt /tmp","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510748_-81001159","id":"20160331-233830_2033647788","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1590"},{"title":"Preview Downloaded Text File","text":"%sh\nhadoop fs -cat /tmp/About-Apache-NiFi.txt | head","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510748_-81001159","id":"20160331-233830_168647264","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1591"},{"text":"%md\nNext we are going to run Spark Python (or PySpark) that can be invoked by prepending a block of Python code with a line containing `%pyspark`.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510748_-81001159","id":"20160331-233830_1923635655","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Next we are going to run Spark Python (or PySpark) that can be invoked by prepending a block of Python code with a line containing <code>%pyspark</code>.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1592"},{"text":"%md\n#\nThe important thing to notice in the next paragraph is the `sc` object or Spark Context. Spark Context is automatically created by your driver program in Zeppelin.\n#\nSpark Context is the main entry point for Spark functionality. A Spark Context represents the connection to a Spark cluster, and can be used to create RDDs, which we will do next.\n#\nRemember that Spark doesn't have any storage layer, rather it has connectors to HDFS, S3, Cassandra, HBase, Hive etc. to bring data into memory. Thus, in the next paragraph you will read data (that you've just downloaded) from HDFS.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510749_-81385908","id":"20160331-233830_178148000","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>The important thing to notice in the next paragraph is the <code>sc</code> object or Spark Context. Spark Context is automatically created by your driver program in Zeppelin.</p>\n<h1></h1>\n<p>Spark Context is the main entry point for Spark functionality. A Spark Context represents the connection to a Spark cluster, and can be used to create RDDs, which we will do next.</p>\n<h1></h1>\n<p>Remember that Spark doesn't have any storage layer, rather it has connectors to HDFS, S3, Cassandra, HBase, Hive etc. to bring data into memory. Thus, in the next paragraph you will read data (that you've just downloaded) from HDFS.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1593"},{"title":"Read Text File from HDFS and Preview its Contents","text":"%pyspark\n\n# Parallelize text file using pre-initialized Spark context (sc)\nlines = sc.textFile(\"/tmp/About-Apache-NiFi.txt\")\n\n# Take a look at a few lines with a take() action.\nprint lines.take(4)\n\n# Output: Notice that each line has been placed in a seperate array bucket.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510749_-81385908","id":"20160331-233830_541232082","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1594"},{"text":"%md\nIn the next paragraphs we will start using Python lambda (or anonymous) functions. If you're unfamiliar with lambda expressions, \nreview **[Python Lambda Expressions](https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions)** before proceeding.","dateUpdated":"Mar 31, 2016 11:47:06 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510749_-81385908","id":"20160331-233830_1894357129","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the next paragraphs we will start using Python lambda (or anonymous) functions. If you're unfamiliar with lambda expressions,\n<br  />review <strong><a href=\"https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions\">Python Lambda Expressions</a></strong> before proceeding.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Mar 31, 2016 11:47:03 PM","dateFinished":"Mar 31, 2016 11:47:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1595"},{"title":"Extract All Words from the Document","text":"%pyspark\n# Here we're tokenizing our text file by using the split() function. Each original line of text is split into words or tokens on a single space.\n#  Also, since each line of the original text occupies a seperate bucket in the array, we need to use\n#  a flatMap() transformation to flatten all buckets into a asingle/flat array of tokens.\n\nwords = lines.flatMap(lambda line: line.split(\" \"))","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510750_-80231661","id":"20160331-233830_2015200328","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1596"},{"text":"%md\nNote that after you click 'play' in the paragraph above \"nothing\" appears to happen.\n#\nThat's because `flatMap()` is a transformation and all transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently – for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n#\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510750_-80231661","id":"20160331-233830_1507315859","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Note that after you click 'play' in the paragraph above &ldquo;nothing&rdquo; appears to happen.</p>\n<h1></h1>\n<p>That's because <code>flatMap()</code> is a transformation and all transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently – for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.</p>\n<h1></h1>\n<p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1597"},{"title":"Take a look at first 100 words","text":"%pyspark\nprint words.take(100)   # we're using a take(n) action\n\n# Output: As you can see, each word occupies a distinc array bucket.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510750_-80231661","id":"20160331-233830_1740542201","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1598"},{"text":"%md\n#### Remove Empty Words\n\nUse `filter()` transformation to remove empty words. Recall that `len()` returns a length of string.\n#\nReplace `<FILL IN>` with appropriate code.","dateUpdated":"Apr 1, 2016 1:40:24 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510751_-80616410","id":"20160331-233830_270532773","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Remove Empty Words</h4>\n<p>Use <code>filter()</code> transformation to remove empty words. Recall that <code>len()</code> returns a length of string.</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Apr 1, 2016 1:40:22 AM","dateFinished":"Apr 1, 2016 1:40:22 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1599"},{"text":"%pyspark\nwordsFiltered = words.filter(<FILL IN>)","dateUpdated":"Mar 31, 2016 11:50:56 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510751_-80616410","id":"20160331-233830_621435112","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1600"},{"title":"Solution (click 'Show output', i.e. closed notepad icon on the right)","text":"%md\n`wordsFiltered = words.filter(lambda w: len(w) > 0)`","dateUpdated":"Apr 10, 2016 12:45:37 AM","config":{"enabled":true,"tableHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510751_-80616410","id":"20160331-233830_1246952023","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>wordsFiltered = words.filter(lambda w: len(w) &gt; 0)</code></p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1601"},{"title":"Get Total Number of Words","text":"%pyspark\n\nprint wordsFiltered.count()     # using a count() action","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510751_-80616410","id":"20160331-233830_229739488","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1602"},{"text":"%md\n#### Word Counts\n\nLet's see what are the most popular words by performing a word count.\nUse `map()` and `reduceByKey()` transformations to create tuples of type (word, count). \n#\nReplace `<FILL IN>` with appropriate code.","dateUpdated":"Mar 31, 2016 11:51:53 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510752_-94852119","id":"20160331-233830_55977510","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Word Counts</h4>\n<p>Let's see what are the most popular words by performing a word count.\n<br  />Use <code>map()</code> and <code>reduceByKey()</code> transformations to create tuples of type (word, count).</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1603"},{"title":"Word count with a RDD","text":"%pyspark\n\nwordCounts = wordsFiltered.map(<FILL IN>).reduceByKey(<FILL IN>)","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"tableHide":false,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510752_-94852119","id":"20160331-233830_216173184","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1604"},{"title":"Solution (click 'Show output', i.e. closed notepad icon on the right)","text":"%md\n`wordCounts = wordsFiltered.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)`","dateUpdated":"Apr 1, 2016 12:38:15 AM","config":{"enabled":true,"tableHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510752_-94852119","id":"20160331-233830_1827856411","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>wordCounts = wordsFiltered.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)</code></p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1605"},{"text":"%md\n#### View Word Count Tuples\nNow let's take a look at top 100 words in descending order with a `takeOrdered()` action.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510753_-95236868","id":"20160331-233830_1029129342","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>View Word Count Tuples</h4>\n<p>Now let's take a look at top 100 words in descending order with a <code>takeOrdered()</code> action.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1606"},{"text":"%pyspark\nprint wordCounts.takeOrdered(100, lambda (w,c): -c)\n","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510753_-95236868","id":"20160331-233830_743558056","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1607"},{"text":"%md\n#### Filter out infrequent words\nUse `filter()` transformation to filter out words that occur less than five times.\n#\nReplace `<FILL IN>` with appropriate code.","dateUpdated":"Apr 1, 2016 1:41:52 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510753_-95236868","id":"20160331-233830_772905299","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Filter out infrequent words</h4>\n<p>Use <code>filter()</code> transformation to filter out words that occur less than five times.</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1608"},{"text":"%pyspark\n\nfilteredWordCounts = wordCounts.filter(<FILL IN>)","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510754_-94082622","id":"20160331-233830_90779590","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1609"},{"title":"Solution (click 'Show output', i.e. closed notepad icon on the right)","text":"%md\n`filteredWordCounts = wordCounts.filter(lambda (w,c): c >= 5)`","dateUpdated":"Apr 11, 2016 8:09:19 PM","config":{"enabled":true,"tableHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510754_-94082622","id":"20160331-233830_1019179760","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>filteredWordCounts = wordCounts.filter(lambda (w,c): c &gt;= 5)</code></p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Apr 1, 2016 1:42:07 AM","dateFinished":"Apr 1, 2016 1:42:07 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1610"},{"title":"Take a Look at Results","text":"%pyspark\n\nprint filteredWordCounts.collect()   # we're using a collect() action to pull everything back to the Spark driver","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510754_-94082622","id":"20160331-233830_1024657848","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1611"},{"title":"","text":"%md\nNow let's use `countByKey()` action for another way of returning a word count.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510755_-94467371","id":"20160331-233830_753086043","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's use <code>countByKey()</code> action for another way of returning a word count.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1612"},{"text":"%pyspark\n\nresult =  words.map(lambda w: (w,1)).countByKey()\n\n# Print type of data structure\nprint type(result)","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510755_-94467371","id":"20160331-233830_1995992930","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1613"},{"text":"%md\nNote that the **result** is an unordered dictionary of type {word, count}.\nSince this is a small set we can apply a simple (non-parallelizeable) python built-in function.\n","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510755_-94467371","id":"20160331-233830_811124723","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Note that the <strong>result</strong> is an unordered dictionary of type {word, count}.\n<br  />Since this is a small set we can apply a simple (non-parallelizeable) python built-in function.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1614"},{"text":"%md\nTake a look at first 20 items in our dictionary.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510756_-96391115","id":"20160331-233830_347028305","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Take a look at first 20 items in our dictionary.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1615"},{"text":"%pyspark\n# Print first 20 items\nprint result.items()[0:20]","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510756_-96391115","id":"20160331-233830_2086620530","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1616"},{"text":"%md\nApply a python `sorted()` function on the **result** dictionary values.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510756_-96391115","id":"20160331-233830_1423292200","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Apply a python <code>sorted()</code> function on the <strong>result</strong> dictionary values.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1617"},{"text":"%pyspark\nimport operator\n\n# Sort in descending order\nsortedResult = sorted(result.items(), key=operator.itemgetter(1), reverse=True)\n\n# Print top 20 items\nprint sortedResult[0:20]","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510757_-96775864","id":"20160331-233830_451661467","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1618"},{"text":"%md\n## Part 2\n#### Introduction to DataFrames and SparkSQL","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510757_-96775864","id":"20160331-233830_1867067371","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 2</h2>\n<h4>Introduction to DataFrames and SparkSQL</h4>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1619"},{"text":"%md\nIn this section we will cover the concept of a DataFrame. You will convert RDDs from a previous section and then use higher level \noperations to demonstrate a different way of counting words. Then you will register a temporary table and perform a word count by \nexecuting a SQL query on that table.\n#\nBy the end of the section you will have learned higher-level Spark abstractions that hide lower-level details, speed up prototyping and execution. ","dateUpdated":"Mar 31, 2016 11:53:42 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510757_-96775864","id":"20160331-233830_770254433","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this section we will cover the concept of a DataFrame. You will convert RDDs from a previous section and then use higher level\n<br  />operations to demonstrate a different way of counting words. Then you will register a temporary table and perform a word count by\n<br  />executing a SQL query on that table.</p>\n<h1></h1>\n<p>By the end of the section you will have learned higher-level Spark abstractions that hide lower-level details, speed up prototyping and execution.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Mar 31, 2016 11:53:40 PM","dateFinished":"Mar 31, 2016 11:53:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1620"},{"title":"DataFrame","text":"%md\nA DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. [See SparkSQL docs for more info](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes).","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510757_-96775864","id":"20160331-233830_634831315","result":{"code":"SUCCESS","type":"HTML","msg":"<p>A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">See SparkSQL docs for more info</a>.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1621"},{"text":"%md\nTransform your RDD into a DataFrame and perform DataFrame specific operations.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510758_-95621617","id":"20160331-233830_911152909","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Transform your RDD into a DataFrame and perform DataFrame specific operations.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1622"},{"title":"Word Count with a DataFrame","text":"%pyspark\n\n# First, let's transform our RDD to a DataFrame.\n# We will use a Row to define column names.\nwordsCountsDF = (filteredWordCounts.map(lambda (w, c): \n                Row(word=w,\n                    count=c))\n                .toDF())\n\n# Print schema\nwordsCountsDF.printSchema()\n\n# Output: As you can see, the count and word types have been inferred without having to explicitly define long and string types respectively.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510758_-95621617","id":"20160331-233830_41054806","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1623"},{"title":"Show top 20 rows","text":"%pyspark\n\n# Show top 20 rows\nwordsCountsDF.show()","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510758_-95621617","id":"20160331-233830_665873755","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1624"},{"title":"Register a Temp Table","text":"%pyspark\n\nwordsCountsDF.registerTempTable(\"word_counts\")","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510758_-95621617","id":"20160331-233830_802915768","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1625"},{"text":"%md\nNow we can query the temporary `word_counts` table with a SQL statement.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510759_-96006366","id":"20160331-233830_1965558675","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now we can query the temporary <code>word_counts</code> table with a SQL statement.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1626"},{"text":"%md\nTo execute a SparkSQL query we prepend a block of SQL code with a `%sql` line.","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510759_-96006366","id":"20160331-233830_403708924","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To execute a SparkSQL query we prepend a block of SQL code with a <code>%sql</code> line.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1627"},{"title":"","text":"%sql\n\n-- Display word counts in descending order\nSELECT word, count FROM word_counts ORDER BY count DESC","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"tableHide":false,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"}}},"editorMode":"ace/mode/sql","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510759_-96006366","id":"20160331-233830_1235044795","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1628"},{"text":"%md\nNow let's take a step back and perform a word count with SQL","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510760_-97930111","id":"20160331-233830_1968421310","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's take a step back and perform a word count with SQL</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1629"},{"title":"Convert RDD to a DataFrame and Register a New Temp Table","text":"%pyspark\n\n# Convert wordsFiltered RDD to a Data Frame\nwordsDF = wordsFiltered.map(lambda w: Row(word=w, count=1)).toDF()","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510760_-97930111","id":"20160331-233830_1271375135","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1630"},{"title":"Use DataFrame Specific Functions to Determine Word Counts","text":"%pyspark\n\n(wordsDF.groupBy(\"word\")\n        .sum()\n        .orderBy(\"sum(count)\", ascending=0)\n        .limit(10).show())","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510760_-97930111","id":"20160331-233830_539606295","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1631"},{"title":"Register as Temp Table","text":"%pyspark\n\n# Register as Temp Table\nwordsDF.registerTempTable(\"words\")","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510761_-98314860","id":"20160331-233830_339558784","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1632"},{"text":"%md\n\nNow perform a word count using a SQL statement against the `words` table and order the results in a descending order by count.\n#\nReplace `<FILL IN>` with appropriate code","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510761_-98314860","id":"20160331-233830_1100432609","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now perform a word count using a SQL statement against the <code>words</code> table and order the results in a descending order by count.</p>\n<h1></h1>\n<p>Replace <code>&lt;FILL IN&gt;</code> with appropriate code</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1633"},{"text":"%sql\n\nSELECT <FILL IN>","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sql","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510761_-98314860","id":"20160331-233830_841691499","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1634"},{"title":"Solution (click 'Show output', i.e. closed notepad icon on the right)","text":"%md\n`SELECT word, count(*) as count FROM words GROUP BY word ORDER BY count DESC`","dateUpdated":"Apr 7, 2016 7:37:03 PM","config":{"enabled":true,"tableHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510762_-97160613","id":"20160331-233830_834240904","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>SELECT word, count(*) as count FROM words GROUP BY word ORDER BY count DESC</code></p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1635"},{"title":"The End","text":"%md\nYou've reached the end of this lab! We hope you've been able to successfully complete all the sections and learned a thing or two about Spark.","dateUpdated":"Apr 1, 2016 12:28:53 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510767_-99084357","id":"20160331-233830_293992216","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You've reached the end of this lab! We hope you've been able to successfully complete all the sections and learned a thing or two about Spark.</p>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","dateStarted":"Mar 31, 2016 11:56:56 PM","dateFinished":"Mar 31, 2016 11:56:56 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1636"},{"text":"%md\n### Additional Resources\nThis is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:\n\n1. [Hortonworks Community Connection](https://hortonworks.com/community/) or HCC -- you'll find guidance, code, examples and best practices to jump start your projects.\n2. [A Lap Around Apache Spark](http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/) Tutorial\n3. [Apache Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n4. [pySpark Reference Guide](https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html)","dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"tableHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510767_-99084357","id":"20160331-233830_1914786212","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Additional Resources</h3>\n<p>This is just the beggining of your journey with Spark. Make sure to checkout these additional useful resources:</p>\n<ol>\n<li><a href=\"https://hortonworks.com/community/\">Hortonworks Community Connection</a> or HCC &ndash; you'll find guidance, code, examples and best practices to jump start your projects.</li>\n<li><a href=\"http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/\">A Lap Around Apache Spark</a> Tutorial</li>\n<li><a href=\"http://spark.apache.org/docs/latest/programming-guide.html\">Apache Spark Programming Guide</a></li>\n<li><a href=\"https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html\">pySpark Reference Guide</a></li>\n</ol>\n"},"dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1637"},{"dateUpdated":"Mar 31, 2016 11:38:30 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1459467510768_-88696137","id":"20160331-233830_200815067","dateCreated":"Mar 31, 2016 11:38:30 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1639"}],"name":"Lab 101: Intro to Spark with Python","id":"2BFGYS3YT","angularObjects":{"2BF4FYFEE":[],"2BEK3QEPR":[],"2BEUGKP8A":[],"2BED2XA8B":[],"2BFC382WH":[],"2BEDYW971":[],"2BGY6N3UD":[],"2BENYVD9X":[],"2BGFTGT6B":[],"2BFYENWB4":[],"2BFDPRBCM":[],"2BDKJZJ55":[],"2BFUWR97N":[]},"config":{"looknfeel":"default"},"info":{}}